# scripts/infer_kobart.py
# í•™ìŠµëœ KoBART(best_model)ë¡œ ìƒˆë¡œìš´ ë¬¸ì„œ ìš”ì•½
# â†’ pdf / txt / csv ì „ë¶€ ì…ë ¥ ê°€ëŠ¥

import os
import argparse

import pdfplumber
import torch
import yaml
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def load_config(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def to_abs(path: str) -> str:
    if os.path.isabs(path):
        return path
    return os.path.join(BASE_DIR, path)


# ===== íŒŒì¼ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ =====
def extract_text_from_file(path: str) -> str:
    ext = os.path.splitext(path)[1].lower()

    # 1) PDF
    if ext == ".pdf":
        texts = []
        with pdfplumber.open(path) as pdf:
            for i, page in enumerate(pdf.pages):
                page_text = page.extract_text() or ""
                page_text = page_text.strip()
                if not page_text:
                    continue
                texts.append(f"[PAGE {i+1}]\n{page_text}")
        return "\n\n".join(texts)

    # 2) TXT
    elif ext == ".txt":
        with open(path, "r", encoding="utf-8") as f:
            return f.read().strip()

    # 3) CSV â†’ ì¼ë‹¨ ì „ì²´ íŒŒì¼ì„ ë¬¸ìì—´ë¡œ ì½ì–´ì„œ ì‚¬ìš©
    elif ext == ".csv":
        with open(path, "r", encoding="utf-8") as f:
            return f.read().strip()

    else:
        raise ValueError(f"[ERROR] ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í™•ì¥ìì…ë‹ˆë‹¤: {ext} (pdf/txt/csvë§Œ ì§€ì›)")


def summarize(
    model,
    tokenizer,
    text: str,
    max_input_len: int,
    max_new_tokens: int,
    min_length: int,
    num_beams: int,
    length_penalty: float,
    no_repeat_ngram_size: int,
) -> str:
    """
    ì‹¤ì œ ìš”ì•½ ìƒì„± í•¨ìˆ˜.
    max_new_tokens, length_penalty ë“±ìœ¼ë¡œ ê¸¸ì´/ìŠ¤íƒ€ì¼ ì œì–´.
    """
    device = model.device

    inputs = tokenizer(
        text,
        max_length=max_input_len,
        truncation=True,
        return_tensors="pt",
    )

    allowed_keys = ["input_ids", "attention_mask"]
    inputs = {k: v.to(device) for k, v in inputs.items() if k in allowed_keys}

    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,      # ğŸ”¥ ìƒˆë¡œ ìƒì„±í•  í† í° ìˆ˜ ê¸°ì¤€
            min_length=min_length,              # ë„ˆë¬´ ì§§ì€ ìš”ì•½ ë°©ì§€
            num_beams=num_beams,
            length_penalty=length_penalty,      # ğŸ”¥ 1.0ë³´ë‹¤ í¬ë©´ ë” ì§§ê²Œ ìš”ì•½í•˜ëŠ” ìª½ ì„ í˜¸
            no_repeat_ngram_size=no_repeat_ngram_size,
        )

    summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    return summary.strip()


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--config",
        type=str,
        default=os.path.join(BASE_DIR, "config", "config.yaml"),
        help="YAML config path",
    )

    # model_dir / ì…ë ¥ íŒŒì¼ ê²½ë¡œ
    parser.add_argument(
        "--model_dir",
        type=str,
        default=None,
        help="train_kobart.pyì—ì„œ ì €ì¥ëœ best_model ë””ë ‰í† ë¦¬ (ë¯¸ì§€ì • ì‹œ config.paths.model_dir ì‚¬ìš©)",
    )
    parser.add_argument(
        "--pdf",
        type=str,   # ì´ë¦„ì€ pdfì§€ë§Œ, ì´ì œ pdf/txt/csv ì „ë¶€ ê°€ëŠ¥
        default=None,
        help="ìš”ì•½í•  íŒŒì¼ ê²½ë¡œ (pdf/txt/csv). ì§€ì • ì•ˆ í•˜ë©´ configì— infer_pdfê°€ ìˆì„ ë•Œ ê·¸ê±¸ ì‚¬ìš©",
    )
    parser.add_argument(
        "--text",
        type=str,
        default=None,
        help="ì§ì ‘ ë„£ëŠ” ì›ë¬¸ í…ìŠ¤íŠ¸ (íŒŒì¼ ëŒ€ì‹ )",
    )

    # generate ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° (Noneì´ë©´ config.yaml ê°’ ì‚¬ìš©)
    parser.add_argument(
        "--max_input_len",
        type=int,
        default=None,
        help="ì…ë ¥ í† í° ìµœëŒ€ ê¸¸ì´ (Noneì´ë©´ config.max_length.input_max_len ë˜ëŠ” generate.max_input_len)",
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=None,
        help="ëª¨ë¸ì´ ìƒˆë¡œ ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ config.generate.max_new_tokens ë˜ëŠ” target_max_len)",
    )
    parser.add_argument(
        "--min_length",
        type=int,
        default=None,
        help="ìš”ì•½ ìµœì†Œ í† í° ê¸¸ì´ (Noneì´ë©´ config.generate.min_length ë˜ëŠ” 30)",
    )
    parser.add_argument(
        "--num_beams",
        type=int,
        default=None,
        help="beam searchì˜ beam ìˆ˜ (Noneì´ë©´ config.generate.num_beams ë˜ëŠ” 4)",
    )
    parser.add_argument(
        "--length_penalty",
        type=float,
        default=None,
        help="ê¸¸ì´ íŒ¨ë„í‹° (1.0ë³´ë‹¤ í¬ë©´ ì§§ê²Œ, ì‘ìœ¼ë©´ ê¸¸ê²Œ ìƒì„±í•˜ëŠ” ê²½í–¥)",
    )
    parser.add_argument(
        "--no_repeat_ngram_size",
        type=int,
        default=None,
        help="ë°˜ë³µ ë°©ì§€ë¥¼ ìœ„í•œ n-gram í¬ê¸° (Noneì´ë©´ config.generate.no_repeat_ngram_size ë˜ëŠ” 3)",
    )

    args = parser.parse_args()

    # ===== config ë¡œë“œ =====
    cfg = load_config(args.config)

    # ----- model_dir -----
    if args.model_dir is not None:
        model_dir = to_abs(args.model_dir)
    else:
        # config.paths.model_dir ì‚¬ìš©
        model_dir_cfg = cfg.get("paths", {}).get("model_dir", None)
        if model_dir_cfg is None:
            print("[ERROR] model_dirì´ ì§€ì •ë˜ì§€ ì•Šì•˜ê³ , config.paths.model_dirë„ ì—†ìŠµë‹ˆë‹¤.")
            return
        model_dir = to_abs(model_dir_cfg)

    if not os.path.exists(model_dir):
        print(f"[ERROR] model_dir ì—†ìŒ: {model_dir}")
        return

    # ----- ìš”ì•½í•  í…ìŠ¤íŠ¸ ì¤€ë¹„ -----
    if args.text is not None:
        src_text = args.text
    else:
        # íŒŒì¼ ê²½ë¡œ ê²°ì •: ì¸ìê°€ ìš°ì„ , ì—†ìœ¼ë©´ config.paths.infer_pdf(ìˆì„ ë•Œë§Œ)
        file_path = None
        if args.pdf is not None:
            file_path = to_abs(args.pdf)
        else:
            infer_pdf_cfg = cfg.get("paths", {}).get("infer_pdf", None)
            if infer_pdf_cfg is not None:
                file_path = to_abs(infer_pdf_cfg)

        if file_path is None:
            print("[ERROR] --pdf / --text / config.paths.infer_pdf ì¤‘ í•˜ë‚˜ëŠ” ì§€ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return

        if not os.path.exists(file_path):
            print(f"[ERROR] íŒŒì¼ ì—†ìŒ: {file_path}")
            return

        print(f"[INFO] íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {file_path}")
        src_text = extract_text_from_file(file_path)

    if not src_text.strip():
        print("[ERROR] ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    # ----- generate ì„¤ì •ê°’ ê²°ì • (config + CLI override) -----
    max_len_cfg = cfg.get("max_length", {})
    gen_cfg = cfg.get("generate", {})

    # max_input_len
    if args.max_input_len is not None:
        max_input_len = args.max_input_len
    else:
        max_input_len = gen_cfg.get(
            "max_input_len",
            max_len_cfg.get("input_max_len", 512),
        )

    # max_new_tokens
    if args.max_new_tokens is not None:
        max_new_tokens = args.max_new_tokens
    else:
        max_new_tokens = gen_cfg.get(
            "max_new_tokens",
            max_len_cfg.get("target_max_len", 256),
        )

    # min_length
    if args.min_length is not None:
        min_length = args.min_length
    else:
        min_length = gen_cfg.get("min_length", 30)

    # num_beams
    if args.num_beams is not None:
        num_beams = args.num_beams
    else:
        num_beams = gen_cfg.get("num_beams", 4)

    # length_penalty
    if args.length_penalty is not None:
        length_penalty = args.length_penalty
    else:
        length_penalty = gen_cfg.get("length_penalty", 1.5)

    # no_repeat_ngram_size
    if args.no_repeat_ngram_size is not None:
        no_repeat_ngram_size = args.no_repeat_ngram_size
    else:
        no_repeat_ngram_size = gen_cfg.get("no_repeat_ngram_size", 3)

    print(f"[INFO] ëª¨ë¸ ë¡œë“œ ì¤‘: {model_dir}")
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()

    print("[INFO] ìš”ì•½ ìƒì„± ì„¤ì •:")
    print(f"  max_input_len      = {max_input_len}")
    print(f"  max_new_tokens     = {max_new_tokens}")
    print(f"  min_length         = {min_length}")
    print(f"  num_beams          = {num_beams}")
    print(f"  length_penalty     = {length_penalty}")
    print(f"  no_repeat_ngram_sz = {no_repeat_ngram_size}")

    print("[INFO] ìš”ì•½ ìƒì„± ì¤‘...")
    summary = summarize(
        model=model,
        tokenizer=tokenizer,
        text=src_text,
        max_input_len=max_input_len,
        max_new_tokens=max_new_tokens,
        min_length=min_length,
        num_beams=num_beams,
        length_penalty=length_penalty,
        no_repeat_ngram_size=no_repeat_ngram_size,
    )

    print("\n==================== SUMMARY ====================")
    print(summary)
    print("=================================================")


if __name__ == "__main__":
    main()
