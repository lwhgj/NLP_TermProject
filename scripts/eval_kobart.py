import os
import sys
import json
import numpy as np
import pandas as pd
import torch
import yaml
from datasets import Dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ===== í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€ =====
# eval_kobart.py (scripts í´ë” ì•ˆ) ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ í´ë”ê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if BASE_DIR not in sys.path:
    sys.path.append(BASE_DIR)
# ==========================================

from src.data_module import make_preprocess_fn
from src.metrics import build_compute_metrics

CONFIG_PATH = os.path.join(BASE_DIR, "config", "config.yaml")



def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def to_abs(path: str) -> str:
    if os.path.isabs(path):
        return path
    return os.path.join(BASE_DIR, path)


def main():
    # ===== config.yaml ë¶ˆëŸ¬ì˜¤ê¸° =====
    cfg = load_config(CONFIG_PATH)

    # max length / batch size
    INPUT_MAX_LEN = cfg["max_length"]["input_max_len"]
    TARGET_MAX_LEN = cfg["max_length"]["target_max_len"]
    TRAIN_BS = cfg["training"]["train_batch_size"]
    BATCH_SIZE = min(TRAIN_BS * 2, 8)   # í‰ê°€ìš©ì€ ë³´í†µ trainë³´ë‹¤ ì¡°ê¸ˆ í‚¤ì›Œë„ ë¨

    # ğŸ”¥ generate ì„¤ì • (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    gen_cfg = cfg.get("generate", {})
    GEN_MAX_NEW_TOKENS = gen_cfg.get("max_new_tokens", TARGET_MAX_LEN)
    GEN_MIN_LENGTH = gen_cfg.get("min_length", 30)
    GEN_NUM_BEAMS = gen_cfg.get("num_beams", 4)
    GEN_LENGTH_PENALTY = gen_cfg.get("length_penalty", 1.5)
    GEN_NO_REPEAT_NGRAM = gen_cfg.get("no_repeat_ngram_size", 3)

    # âœ… ê²½ë¡œëŠ” YAMLì—ì„œ ì½ê¸°
    paths_cfg = cfg.get("paths", {})
    MODEL_DIR_RAW = paths_cfg.get("model_dir")
    TEST_CSV_RAW = paths_cfg.get("test_csv")

    if not MODEL_DIR_RAW:
        print("[ERROR] config.paths.model_dir ì´(ê°€) ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return
    if not TEST_CSV_RAW:
        print("[ERROR] config.paths.test_csv ì´(ê°€) ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    MODEL_DIR = to_abs(MODEL_DIR_RAW)
    TEST_CSV = to_abs(TEST_CSV_RAW)

    print(f"[INFO] CONFIG loaded")
    print(f" - input_max_len       = {INPUT_MAX_LEN}")
    print(f" - target_max_len      = {TARGET_MAX_LEN}")
    print(f" - train_batch         = {TRAIN_BS}")
    print(f" - eval_batch          = {BATCH_SIZE}")
    print(f" - model_dir           = {MODEL_DIR}")
    print(f" - test_csv            = {TEST_CSV}")
    print(f" - gen.max_new_tokens  = {GEN_MAX_NEW_TOKENS}")
    print(f" - gen.min_length      = {GEN_MIN_LENGTH}")
    print(f" - gen.num_beams       = {GEN_NUM_BEAMS}")
    print(f" - gen.length_penalty  = {GEN_LENGTH_PENALTY}")
    print(f" - gen.no_repeat_ngram = {GEN_NO_REPEAT_NGRAM}")

    # ===== ê²½ë¡œ ì²´í¬ =====
    if not os.path.exists(MODEL_DIR):
        print(f"[ERROR] model_dir ì—†ìŒ: {MODEL_DIR}")
        return
    if not os.path.exists(TEST_CSV):
        print(f"[ERROR] test_csv ì—†ìŒ: {TEST_CSV}")
        return

    # ===== ë°ì´í„° ë¡œë“œ =====
    df = pd.read_csv(TEST_CSV)
    if "text" not in df.columns or "summary" not in df.columns:
        print("[ERROR] test_csvì—ëŠ” 'text', 'summary' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    test_ds = Dataset.from_pandas(df)

    # ===== ëª¨ë¸ ë¡œë“œ =====
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)
    model.eval()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    # ===== ì „ì²˜ë¦¬ =====
    preprocess_fn = make_preprocess_fn(
        tokenizer,
        input_max_len=INPUT_MAX_LEN,
        target_max_len=TARGET_MAX_LEN,
    )
    test_ds_token = test_ds.map(
        preprocess_fn,
        batched=True,
        remove_columns=test_ds.column_names,
    )

    # í…ì„œ í¬ë§·
    test_ds_token.set_format(
        type="torch",
        columns=["input_ids", "attention_mask", "labels"],
    )

    loader = DataLoader(test_ds_token, batch_size=BATCH_SIZE)

    all_preds, all_labels = [], []

    print("[INFO] í…ŒìŠ¤íŠ¸ ì…‹ ìš”ì•½ ìƒì„± ì¤‘...")

    for batch in loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].numpy()

        with torch.no_grad():
            gen_ids = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                # ğŸ”¥ ê¸°ì¡´: max_length=TARGET_MAX_LEN
                # â†’ ì´ì œ config.generate ê¸°ë°˜ìœ¼ë¡œ ìƒì„± ê¸¸ì´/ìŠ¤íƒ€ì¼ í†µì¼
                max_new_tokens=GEN_MAX_NEW_TOKENS,
                min_length=GEN_MIN_LENGTH,
                num_beams=GEN_NUM_BEAMS,
                length_penalty=GEN_LENGTH_PENALTY,
                no_repeat_ngram_size=GEN_NO_REPEAT_NGRAM,
            )

        all_preds.append(gen_ids.cpu().numpy())
        all_labels.append(labels)

    all_preds = np.concatenate(all_preds, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)

    compute_metrics = build_compute_metrics(tokenizer)
    metrics = compute_metrics((all_preds, all_labels))

    print("\n========== TEST METRICS ==========")
    print(json.dumps(metrics, indent=2, ensure_ascii=False))
    print("==================================")


if __name__ == "__main__":
    main()
